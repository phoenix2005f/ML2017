{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        \n",
    "    def train(self,X,y,batch_size=None,epoch=1000,l_rate=0.3):\n",
    "        self.X = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        self.y = y\n",
    "        self.w = np.zeros( (self.X.shape[1],1) )\n",
    "        s_gra = np.zeros( (self.X.shape[1],1) )\n",
    "        \n",
    "        num_data = X.shape[0]\n",
    "        \n",
    "        if not batch_size:\n",
    "            batch_size = self.X.shape[0]\n",
    "        \n",
    "        num_batch = math.ceil(num_data/batch_size)\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            # w,y_plum,s_gra = self.__update(l_rate,s_gra)\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                \n",
    "                X_batch = X[i*batch_size:min((i+1)*batch_size,num_data)]\n",
    "                y_batch = y[i*batch_size:min((i+1)*batch_size,num_data)]\n",
    "                hypo = np.dot(X_batch,self.w) \n",
    "                y_predict = self.sigmoid(hypo)\n",
    "                error = y_batch - y_predict\n",
    "\n",
    "                gra= -1*np.dot(X_batch.T,error)\n",
    "\n",
    "\n",
    "                s_gra = s_gra + gra**2  \n",
    "            \n",
    "                self.w = self.w - l_rate/np.sqrt(s_gra)*gra\n",
    "                \n",
    "\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        z = np.exp(z)\n",
    "        total = np.sum(z)\n",
    "        return z/total\n",
    "    \n",
    "    def cost_function(self,y_hat,y):\n",
    "        cost_sum=0\n",
    "        for i,val in enumerate(y_hat):\n",
    "            if val[0]==1:\n",
    "                cost_sum+=y[i][0]\n",
    "            elif val[0]==0:\n",
    "                cost_sum+=(1-y[i][0])\n",
    "                \n",
    "        return -1*cost_sum\n",
    "                \n",
    "        \n",
    "    def cost_function_derivative(self,X,y_hat,y_sig):\n",
    "        diff = y_hat - y_sig\n",
    "        grad = -1*np.dot(X.T,diff)\n",
    "        return grad\n",
    "        \n",
    "    \n",
    "        \n",
    "    def predict(self,test_X):\n",
    "        test_X = np.hstack((np.ones((test_X.shape[0],1)),test_X))\n",
    "        y_plum = np.dot(test_X,self.w)\n",
    "        #temp_y = self.sigmoid(y_plum)\n",
    "        res = [1 if i>=0 else 0 for i in y_plum]\n",
    "        #res = [1 if i>=0.5 else 0 for i in temp_y]\n",
    "\n",
    "        return res\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        res = 1/(1.0+np.exp(-z))\n",
    "        return np.clip(res,0.00000000000001,0.99999999999999)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def standarize(df):\n",
    "        res = df\n",
    "        cols=res.columns\n",
    "        for col in cols:\n",
    "            max_num = res[col].max()\n",
    "            min_num = res[col].min()\n",
    "            diff = max_num-min_num\n",
    "            res[col] = (res[col]-min_num)/diff\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def standarize_std(df):\n",
    "        res=df\n",
    "        cols = res.columns\n",
    "        for col in cols:\n",
    "            std = np.std(res[col])\n",
    "            mean = np.mean(res[col])\n",
    "            res[col]=(res[col]-mean)/std\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = income[\"income\"]==' <=50K'\n",
    "y_train = np.array([0 if i else 1 for i in y_train]).reshape((-1,1))\n",
    "\n",
    "X_train = income.drop(\"income\",axis=1)\n",
    "all_data = pd.concat([X_train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discrete_col = ['workclass','education','marital_status','occupation', 'relationship', 'race','native_country']\n",
    "continuous_col = ['age','fnlwgt','sex','capital_gain', 'capital_loss','hours_per_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discrete = pd.get_dummies(all_data[discrete_col])\n",
    "continuous = all_data[continuous_col]\n",
    "data = pd.concat([continuous,discrete],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sex_mapping={\n",
    "    ' Male':0,\n",
    "    ' Female':1\n",
    "}\n",
    "\n",
    "data[\"sex\"]=data[\"sex\"].map(sex_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = ['age','fnlwgt','capital_gain','capital_loss','hours_per_week']\n",
    "#data = data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "data=lr.standarize_std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[:income.shape[0],:].values\n",
    "X_test = data.iloc[income.shape[0]:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.train(X_train,y_train,batch_size=600,epoch=100,l_rate=0.4)\n",
    "predictions=lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=pd.read_csv(\"correct_answer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8526503286038941\n"
     ]
    }
   ],
   "source": [
    "ans_list=list(ans[\"label\"])\n",
    "count=0\n",
    "for i,val in enumerate(ans_list):\n",
    "    if val == predictions[i]:\n",
    "         count+=1\n",
    "rate = count/len(ans_list)\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    total = np.sum(z)\n",
    "    return z/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09003057],\n",
       "       [0.24472847],\n",
       "       [0.66524096]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1],[2],[3]]\n",
    "softmax(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phx/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lr = LogisticRegression()\n",
    "test_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
